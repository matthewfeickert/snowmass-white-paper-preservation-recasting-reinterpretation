\documentclass[11pt]{article}

\input{latex/packages.tex}

% basic data for the eprint:

\textwidth=6.0in  \textheight=8.5in

% Adjust these for your printer:
\parskip=0.1truein 

% preprint number data:
\newcommand\pubnumber{DRAFT}
\newcommand\pubdate{\today}

\input{latex/commands.tex}

\input{latex/workshopsymbols.tex}

% Toggle commenting to disallow/allow line numbers.
\linenumbers

% DEADLINES
% 15 January 2022: Please provide an abstract and short outline of your white paper contribution to TF07 conveners
% 23-25 February 2022: Theory Frontier Conference at the KITP (waiting for further notice from KITP)
% 15 March 2022: contributed papers (``white papers”) due 
% 31 May 2022: draft topical group reports - release for public comments 
% 30 June 2022: draft Frontier report - release for public comments 
% 17-27 July 2022: Seattle meeting (``Snowmass Community Study”)
% 30 September 2022: final reports of TGs and Frontiers
% 31 October 2022: Snowmass book including high level executive summaries online

% N.B.:
% CompF7 scope, mandate, questions:
% **White papers are encouraged to touch on these topics if applicable**
% * Functional areas
%    - Public data (comes in many forms … \hepdata, public likelihoods, CERN OpenData, data for education/outreach)
%       + Tools for generating annotated public data and software
%    - Tools for combining results across experiments and frontiers
%    - Tools for archiving and re-running the analysis (RECAST/REANA)
% * Mandate
%   - Define the stakeholders and consumers of the data and software
%   - What are the needs/requirements of the stakeholders?
%   - What resources are needed?
%       + e.g. long-term storage with external access, infrastructure for preserving executable code, etc. 
%       + metadata infrastructure
%   - What technologies are available or will be available, what is the technology evolution of these tools?
%       + Discussed in common with CompF5: End User Analysis:
%          * version control
%          * Containers/VMs
%          * proprietary software/licenses
%   - How are/will the stakeholders use these technologies?
%   - How are other science domains handling this topic?
%   - What are the workflows that are used to combine results across experiments and frontiers?
%   - What tools are used/needed by the stakeholders to combine results across experiments and frontiers?
%   - What is the technology evolution of these tools?
%   - What are other science domains using, what is industry using?

% **Goals of the white paper**
% * To put forward one primary finding / comment / recommendation (and perhaps a few secondary ones), with the hope that it makes it into the Frontier summary and has some impact on the overall executive summary 
% * Provide some supporting material for those recommendations
% * Have relevant names attached to that document so that it is a credible source

% **Running meeting notes**
% Google Doc: https://docs.google.com/document/d/1WXM7BpQ_ROgqdfu2HNGSTOQ83--p6sUt46oJpdq30-A/edit?usp=sharing




\begin{document}

\pubblock

\snowmass{}

\Title{\textbf{Data and Analysis Preservation,\\ Recasting, and Reinterpretation}}

\begin{center}{\large
TF07 (Collider Phenomenology in the Theory Frontier)\\
COMPF7 (Reinterpretation and long-term preservation of data and code)}
\end{center}

%\medskip

% IF YOU DON'T SEE YOURSELF ON THE AUTHOR LIST ADD YOURSELF
% ALSO, IF YOU'D LIKE, PLEASE ADD YOUR ORCID TOO

% Author List
\begin{center}
% name\,\orcidlink{0000-XXXX-XXXX-XXXX}\textsuperscript{1},
Stephen Bailey\,\orcidlink{0000-0003-4162-6619}\textsuperscript{1},  % Endorser
Christian~Bierlich\,\orcidlink{0000-0002-3978-6085}\textsuperscript{17},
Andy~Buckley\,\orcidlink{0000-0001-8355-9237}\textsuperscript{14},
% Jon~Butterworth\,\orcidlink{0000-0002-5905-5394}\textsuperscript{16}
Kyle~Cranmer\,\orcidlink{0000-0002-5769-7094}\textsuperscript{2},
% Nishita~Desai\,\orcidlink{0000-0001-7942-1649}\textsuperscript{3},
Matthew~Feickert\,\orcidlink{0000-0003-4124-7862}\textsuperscript{4},
% Lukas~Heinrich\,\orcidlink{0000-0002-4048-7584}\textsuperscript{5},
% Axel~Huebl\,\orcidlink{0000-0003-1943-7141}\textsuperscript{1},
% Matias~Carrasco Kind\,\orcidlink{0000-0002-4802-3194}\textsuperscript{4},
Sabine~Kraml\,\orcidlink{0000-0002-2613-7000}\textsuperscript{6},
Anders~Kvellestad\,\orcidlink{0000-0002-5267-7705}\textsuperscript{18},
Clemens~Lange\,\orcidlink{0000-0002-3632-3157}\textsuperscript{7},
Andre~Lessa\,\orcidlink{0000-0002-5251-7891}\textsuperscript{8},
Kati~Lassila-Perini\,\orcidlink{0000-0002-5502-1795}\textsuperscript{9},
% Christine~Nattrass\,\orcidlink{?}\textsuperscript{?},
Mark~S.~Neubauer\,\orcidlink{0000-0001-8434-9274}\textsuperscript{4},
% Harrison~B.~Prosper\,\orcidlink{0000-0002-4077-2713}\textsuperscript{10},
Sezen~Sekmen\,\orcidlink{0000-0003-1726-5681}\textsuperscript{11},
% Giordon~Stark\,\orcidlink{0000-0001-6616-3433}\textsuperscript{12},
% Wolfgang~Waltenberger\,\orcidlink{0000-0002-6215-7228}\textsuperscript{13},
Graeme~Watt\,\orcidlink{0000-0003-0775-6604}\textsuperscript{15}
\end{center}

% Affiliations 
\begin{center}
\textbf{1}~Lawrence Berkeley National Laboratory, USA
\textbf{2}~New York University, USA
\textbf{3}~Tata Institute of Fundamental Research, India
\textbf{4}~University of Illinois at Urbana-Champaign, USA
\textbf{5}~Technische Universität München, Germany
\textbf{6}~Univ. Grenoble Alpes, CNRS, Grenoble INP, LPSC-IN2P3, 38000 Grenoble, France
\textbf{7}~Paul Scherrer Institute, Villigen, Switzerland
\textbf{8}~Universidade Federal do ABC, Brazil
\textbf{9}~Helsinki Institute of Physics, Finland
\textbf{10}~Florida State University, USA
\textbf{11}~Kyungpook National University, Korea
\textbf{12}~SCIPP, UC Santa Cruz, CA, USA
\textbf{13}~HEPHY and University of Vienna, Austria
\textbf{14}~University of Glasgow, UK
\textbf{15}~IPPP, Durham University, UK
\textbf{16}~University College London, UK
\textbf{17}~Lund University, Lund, Sweden
\textbf{18}~University of Oslo, Norway
\end{center}

\begin{Abstract}
\noindent We make the case for the systematic, reliable preservation of data, data products, and analysis code for their long-term future reuse, in order to make the most out of particle physics experiments.
We cover the needs of both the experimental and theoretical particle physics communities, and outline the goals and benefits that are uniquely enabled by analysis recasting and reinterpretation. 
We discuss the technical challenges and infrastructure needs, as well as sociological challenges and changes, and give summary recommendations to the particle physics community.
\end{Abstract}
% Sabine @ Kyle et al: your may want to improve :-)

\clearpage

\tableofcontents

%-------------------------------------------------------------
\section{Introduction}
%-------------------------------------------------------------

\noindent\textbf{Writers:} Matthew, Kyle, \ldots

What is needed to re-analysis and reuse analyses from the LHC experiments and other collaborations is to explore not-yet-thought-of theories in the long-term future (i.e., on the 10 to 50-year time scale).
Exploring these theories requires developing long-term storage and preservation standards of analysis data products. The infrastructure and corresponding techniques must be built up, including reproducible workflows for recasting and reinterpreting analysis data products. Finally, significant sociological changes in the broader particle physics community need to happen.

\TODO{Technical considerations along the lines of \cite{LHCReinterpretationForum:2020xtr,Cranmer:2021urp}, etc.}

\TODO{Note that impact of analysis preservation goes beyond BSM searches, e.g.~testing and tuning of strong-interaction models at EIC.}

Q: To what extent do we discuss differences between high- and low-energy experiments (energy vs.~intensity frontier), particle and astroparticle communities, and other fields?


\NB{Idea: Call out the main thrusts of the white paper in the introduction:}

\ALERT{Kyle: High-level recommendations that are bubbling up: \\
- Emphasize getting the most out of the data. \\
- We need to concentrate on the long-term vision for the next decade and for us to describe that in terms of community action/support. \\
- Incorporate into operations and facilities planning for experiments. (Jon points out that need ownership beyond the experiments, including the phenomenology community as well. Think about an experiment-independent platform (which resonates with Cosmo frontier white paper). Motivates declarative specifications, interfaces, and standards.)} 

\TODO{Preservation and sharing beyond a single experiment can be a $\Delta$ of effort on top of an experiment's core effort that becomes hard to resource and which may not be well recognized. Appropriate resources and career structures to support such work, which benefit the whole theory/phenomenology/experiment community, need to be planned and provided.}

\begin{itemize}
    \item \ldots
    \item Open infrastructure
    \item Data preservation (and what that means)
\end{itemize}

\NB{stress that while we focus on the LHC, our findings apply to particle physics experiments in general, including low-energy, not collider, ...}


\TODO{G. Watt: Would it be helpful to use the \href{https://opendata.cern.ch/docs/about}{DPHEP categories} from Level 1 (publication-related data) to Level 4 (raw event-level data)? SK: I don't think so; imo the DPHEP structure is not really appropriate for the points we make below. KLP: as discussed, I agree, I've commented on the DPHEP levels and added a brief introduction to what follows. See the paragraph below.}

In the following, we distinguish between preserving collision and simulated event data used as input to physics analyses, described in Section~\ref{data-preservation}, and preserving workflows and data products connected to specific analyses, covered in Sections~\ref{analysis-preservation} and~\ref{data-products}, respectively. Section~\ref{reinterpret} addresses using existing data products in reinterpreting new physics scenarios.

%\QUESTION{Kati to Matthew et al.: Does the brief introduction paragraph above address the "Todo" comment below? The paragraph can be expanded if needed, but it would be best to keep it in the context of this paper.}

\TODO{Given the general level of confusion in the larger community RE: the terms ``data products'', ``data'' vs. ``derived data'', ``data preservation'' vs. ``analysis preservation'' and others, it might make sense to create a figure that tries to put the high-level concepts into some diagram to be understood pictorially. Matthew will self assign this for next week (2022-02-23) but will welcome feedback/getting scooped by others with more excellent skill here (e.g., Lukas).}


%\TODO{Kati: The following is (loosely) the wording of the DPHEP levels from the CERN open data policy. If we keep this here, it will make sense to change the order of the sections in the paper. "4. Preservation of data (analysis) products" is Level 1 and "2. Data preservation (open data)" is Level 3, and "3. Analysis preservation" is in between}

% The Data Preservation has identified four levels of complexity of HEP data and Long Term Analysis in High Energy Physics (DPHEP) Study Group FIXME cite.
% The LHC experiments define the data management plans to address the long-term preservation of internal data products. See Akopov et al., Status report of the DPHEP Study Group: Towards a global effort for sustainable data preservation in high energy physics. arXiv preprint arXiv:1205.4667 (2012).
% \begin{itemize}
% \item Published Results (Level 1): 
% \begin{itemize}
%     \item peer-reviewed publications
%     \item additional information and data made available at the time of publication such as simplified or full binned likelihoods, as well as unbinned likelihoods based on datasets of event-level observables extracted by the analyses
%     \item event selection routines stored in specialized tools.
% \end{itemize}
% \item Outreach and Education (Level 2): 
% \begin{itemize}
%     \item dedicated subsets of data in simplified, portable and self-contained formats suitable for educational and public understanding purposes
%     \item lightweight environments to allow the easy exploration of these data.
% \end{itemize}
% \item Reconstructed Data (Level 3): 
% \begin{itemize}
%     \item calibrated reconstructed data with the level of detail useful for algorithmic, performance, and physics studies
%     \item appropriate simulated data samples
%     \item provenance metadata
%     \item software 
%     \item additional information sufficient to allow high-quality analysis, including the main correction factors and corresponding systematic uncertainties related to calibrations, detector reconstruction, and identification
%     \item reproducible example analysis workflows, and documentation
%     \item virtual computing environments compatible with the data and software.
% \end{itemize}
% \item Raw Data (Level 4): 
% \begin{itemize}
%     \item raw data before the reconstruction step
%     \item full machinery to reconstruct Level 3 data starting from raw data.
% \end{itemize}
% \end{itemize}


%-------------------------------------------------------------
\section{Data preservation (open data)}
\label{data-preservation}
%-------------------------------------------------------------

[status, what data is currently open (actual and Monte Carlo), plans by collaborations, FAIR Open Data, infrastructure needs, ...]\\

\noindent\textbf{Writers:} Kati, Mark?

%NOTE (from Kati): this is getting too long, and it is not even done. Let me know if I should cut it down.
%Reply (from Sabine): i.m.o.\ this is a very important section; it merits a couple of pages. I would not cut it down, not at this stage. 
% OK, thanks. I'm done. It is CMS-centric as usual, but I guess it is justified. Feel free to adapt.

% \COMMENT{Andy: ``Data'' is being used throughout here to mean ``events'', either actual or simulated. We should be more explicit about this since there are many categories of ``data'', and here we specifically mean event-wise data at some level. The ``data products'' described later are also typically data\dots maybe ``derived data'' would be better for them.
% \hrulex
% Kati: Agree that this would benefit from clarification. In the CERN open data portal terminology, we use ``derived data'' for datasets that have been produced (skimmed or slimmed or else) from the existing public data. Maybe we could use ``data analysis products''? The distinction would be best placed in the introduction.
% } 

Following the positive experience and feedback from the open releases of recorded and simulated event-level datasets by the CMS experiment since 2014, CERN formulated an open-data policy for the LHC experiments~\cite{cern-data-policy} in 2020. As the host laboratory, CERN maintains and develops the infrastructure needed for the portal, provides storage resources, and takes the custodial responsibility of released open data in the long term. All LHC experiments are committed to releasing research-quality data through the CERN Open Data portal~\cite{CODP}. The amount of data and release timeline varies from one experiment to another~\cite{cern-data-policy,cern-open-data-privacy-policy,cms-open-data-policy,atlas-open-data-policy,lhcb-open-data-policy,alice-open-data-policy}. 

CMS is the only experiment having released research-quality data at the time of writing. The CMS open-data releases contain complete reprocessing of collision data from each data-taking period and the simulated data corresponding to these data. They are made available in the format and with the exact data quality requirements used by analyses of the CMS collaboration. The volume of data, both actual and Monte Carlo simulation, amounts currently to 2.8~PB. CMS also provides a compatible version of the CMSSW and additional information necessary to perform a research-level physics analysis on the public data. The documentation comes with example code and specific guide pages to instruct new users.

Data released through the CERN open-data portal satisfy Findable, Accessible, Interoperable, and Reusable (FAIR) principles for scientific data management~\cite{FAIR-paper} for data and metadata to a large extent. Due to the complexity of experimental particle physics data, the FAIR principles alone do not guarantee the reusability of these data, and additional effort is needed to pass on the knowledge needed to use and interpret them correctly. In large experiments, collaboration members benefit from the existing knowledge infrastructure (meetings, discussion forums, mentoring, specific groups responsible for providing data assets needed for physics analysis, and more) that is unavailable to external users or long-term. 

%[more details can be added here about challenges related to computing and software, and those related to intricacies of experimental particle physics data]

Early testing is of utmost importance to ensure that the open data is ready for physics research and that all necessary additional information is captured, stored, and provided. Therefore, CMS emphasizes releasing data relatively early after the data-taking. External users' feedback and questions indicate the eventual missing data assets and lack of information. The release can complement them while the expertise on these data is still present in the collaboration. To encourage the use of open data and to get direct feedback, the CMS open-data group is organizing regular workshops with hands-on tutorials. It is essential to reach out to a diverse user community, from newcomers to seniors with different computing backgrounds and physics skills. While the CMS open-data group is looking forward to organizing the next workshop in presence, the value of remote, virtual events with many participants who would not have been able to travel is acknowledged.

Setting up example analysis workflows is one of the priorities for making open data reusable. They should demonstrate all necessary steps from data access and selection to any eventual corrections and address the particularities of experimental data, such as estimating efficiencies and uncertainties. Activities for preserving analysis workflows internally in the collaboration will facilitate building such workflows. In use with open data, the software container technology is suitable for automating workflows. The experience gained with them can be used for current analyses within the collaboration.

%[input from OD users for these workflows, i.e., they may have already repeated an existing  measurement with OD as a test]

%Can someone check this please:
There are many successful data preservation efforts in the LEP and HERA experiments. However, no public access to these data exists in terms defined in the CERN data policy for the LHC experiments. Access to preserved data is made possible through different modalities, such as joining or working with collaboration members. The BABAR collaboration has decided to make its data available for future analyses through the CERN Open Data portal. Lack of person-power is often a bottleneck, and while valid for experiments in the data-taking phase, it is even more evident for experiments past data taking. Open access initiatives have to start at an early stage.

%[OD beyond the LHC? Reports in DPHEP workshop https://indico.cern.ch/event/1043155/timetable/ BaBar decision in p 10 of https://indico.cern.ch/event/1043155/contributions/4383112/attachments/2267926/3850947/BaBar-DPHEP-20210621.pdf#page=10 Difficulty of policy-making after active data taking? ]

Building a community of users for these data is paramount to making open-data initiatives successful. The theory community is a critical player in this, and promoting open data despite the substantial work required in their analysis is a strong demonstration of their value and an invaluable validation effort for their usability for future generations.

\noindent
\textbf{Recommendations:}
\begin{description}
   \item[2.1:] Agree on data preservation and public event-data releases as a means to maximize scientific outcomes, and allocate resources and responsibilities to achieve this goal in the experiments' organization. %\ALERT{Kyle: incorporate budgeting/resources and experiment organization. Incorporate into operations and facilities. Kati: see updated 2.1 & 2.3  }
   \item[2.2:] Give long-term custodial responsibility of public data to the host laboratory or an organization that persists beyond the experiment's lifetime and uses common distribution platforms with other experiments.
   \item[2.3:] Incorporate preparing data for public releases and invest in preserving the knowledge needed for their use in the data processing and analysis operations and facilities.
   \item[2.4:] Encourage and promote the use of open data to explore and improve usability and to ensure that all necessary information for research-level use is available.
\end{description}



%-------------------------------------------------------------
\section{Analysis preservation}
\label{analysis-preservation}
%-------------------------------------------------------------

\noindent\textbf{Writers:} Matthew, Clemens, Lukas, Sabine, Sezen, Andy \dots + Christian Bierlich, Jon B, Andrii V, Benjamin Fuks?

% \ALERT{
%   \NB{make recommendations for \textbf{generic ideas}, but can give \textbf{specific examples with specific tools}}
%   \NB{Can focus and compare and contrast the different levels of analysis preservation.}
% }
% [scope, status, difficulties met, person power and infrastructure needs, ....]

Preservation of event-wise analysis logic and workflows in any specific form enables the reuse of the original analysis process and associated data products. Such reuse may, for instance, be for (re)interpretation in terms of physics models not considered in the original paper by the experimental collaboration; this may range from beyond-Standard-Model theories to new ideas and implementations non-perturbative QCD, and everything in-between.

As such, an experiment needs to integrate analysis preservation into its publication processes alongside open-data and data-product preservation to achieve its full scientific impact. Without this, the influence of the hundreds of published analyses from the LHC, HL-LHC, EIC, and other modern collider experiments will be limited mainly to the physics ideas in vogue at the time the collaboration collected collider data. The public investment in experimental programs underscores the importance of going beyond the first publication and ensuring that analyses continue providing scientific value in perpetuity.
% demands it also be usable in perpetuity, constrain new models, or analyze old ones in more depth as complementary developments narrow down viable model spaces.

Different levels of event-wise analysis preservation are possible. They already exist, from full-detail preservation of the experimental analysis workflow to ``fast'' or ``lightweight'' emulations of the entire analysis that emphasize speed (and often greater public availability) to the expense of some precision. Both the accurate-but-expensive and fast-but-approximate approaches (and points in-between) have value for physical applications. Given an ample model space to explore, combining many preserved event analyses to obtain a more statistically significant result, the emphasis lies more on speed and efficiency than total accuracy, so a rough measure of which parameter regions are viable. However, the emphasis naturally shifts to precision with a smaller range of models to explore, perhaps after such fast-interpretation triage. The expense of re-running full experimental software stacks becomes justifiable and tractable.

In the following sections, we review first the full-detail and then the lightweight preservation paradigms, and finally the current status and issues of both, in both the high-energy $pp$ and heavy-ion programs.

% \COMMENT{From Christian Bierlich: I would suggest some introductory text about the current state-of-the-art in different sub-fields. While high-energy $pp$ has come very far, with even advanced recasting, HEHI is still in its infancy and still need discussion of standard + basic notions of reproducibility in place, i.e.~need of much more person power}

%\subsection{Collaboration-internal preservation} -- the principled distinction is less internal/public and more about the level of detail preserved / CPU cost of running it
\subsection{Full-detail preservation} 

The most faithful approach to analysis preservation is to store, in a re-runnable form, the exact software chain used to perform the analysis in the experiment. As each experiment has its internal data formats and software frameworks, injecting new models for analysis typically requires running the full post-generation stack from detector simulation and reconstruction to the physics-analysis code.

The obvious consequence is that simulation and reconstruction are computationally expensive, so is event reinterpretation via such analysis preservations. A secondary implication is the Intellectual Property: at the time of writing, the ability to perform detailed simulation of an experiment's detector response is strategically sensitive information and hence non-public. While not unavoidable, e.g.~by, staged release of older data and frameworks, full-detail preservations exist for collaboration-internal use only.

Preserving full-detail analysis chains is complicated by the diversity of analysis software frameworks, even within a given experiment. Well-versioned central production systems within one framework typically perform event generation, simulation, reconstruction, and data reduction (``derivation''). By contrast, most physics analyses within an experiment start from derived data tiers, and the lack of constraints has led to a proliferation of alternative analysis frameworks. These often lack extensive documentation, may be version-controlled in different locations, and without coordination \& standardization, naturally, evolve as many incompatible technical interfaces as there are packages.

Therefore, for reproducibility, experimental collaborations and host labs must arrange to store analysis data in well-defined repositories and structures, with a plan for long-term archiving. The host institutions should monitor the correctness and continuing validity of the software by using continuous integration testing on a range of suitable datasets for each analysis. As a part of these tests, Linux container images (e.g.~Docker~\cite{docker}, Podman~\cite{podman}, or Singularity~\cite{singularity}) should be built and preserved on container registries to capture the entire software environment needed for data processing.

Physics analyses typically consist of multiple, separately executed steps (e.g.~event, selection and signal extraction). It is also important to capture the workflows for running and combining them over multiple event samples. There is currently no \emph{de~facto} standard for this workflow language across the experimental community: for example, Common Workflow Language~\cite{CWL}, Snakemake~\cite{SnakeMake}, Yadage~\cite{Cranmer:2017frf,yadage_code}, and Argo Workflows~\cite{argo} are in use by different collaborations. Standardization and evolution around a smaller (or unique) subset of such languages would improve interoperability and knowledge transfer. In making such a choice, we stress the importance of a declarative rather than imperative model~\cite{10.3389/fdata.2021.661501}, as the former enables researchers to concentrate on the physics task with minimal need to consider technical details such as scalable job orchestration. Developers can significantly reduce the complexity of workflow descriptions in full-detail preservation by establishing standard interfaces for tools implementing processing steps so that they can be more easily composed.

\TODO{Matthew: Do preserved analyses provide enough metadata that workflow tools can figure out commonalities of simulation and reconstruction versions/geometries between analyses and avoid repeating common steps?}

\TODO{Standardising output format for direct comparability to \hepdata data products}

\TODO{Comment on the ability/limitations/approximations for ``updating'' contributing processes in preserved analyses. I.e.~there are clear use-cases for updating either a background process (nominal or whole-model) to a more precise one published since the original publication or to update or inject a signal model to any number of alternative hypotheses. It would require re-fitting for a perfect result, but even a no-refit approximation would be valuable.}

% In ATLAS, there are ongoing efforts for complete preservation of the experiment analyses in the Supersymmetry, Exotics, and Higgs and Diboson working groups through the \recast system implemented through \href{https://github.com/recast-hep/recast-atlas}{recast-atlas} (\NB{Currently have \texttt{recast-atlas} under collaboration-internal instead of public, as even though the code is open source you still need EOS access at CERN. So it is more ``CERN public'' than ``true public''.})


\subsection{Lightweight preservation}
\label{lightweight}

The approach described above represents a high-fidelity version of preservation, with implications for the original analysis software chain and typically high computing-power requirements on any large-scale reuse of the preserved software stack. Even for organizations with the computational resources of active HEP experiments --- and at present such preservations are in-house --- physicists cannot use this form of preservation to explore large model-parameter spaces, e.g.~$\gg 2$ dimensions via adaptive samplers.

These limitations motivate a more lightweight form of preservation, with more modest CPU demands. It is simplest to make this alternative form independent of the original experiment software chain, with the desirable side-effect that it is also simpler to release publicly. The cost of being lightweight is that the approximations involved reduce the precision of the preservation; this makes lightweight preservation suitable for identifying model regions of interest rather than for making definitive statements of discovery. Public availability is essential for theorists' studies outside the experimental collaborations, such as testing new theoretical ideas against the data from several analyses and experiments in a global approach.

Several lightweight frameworks have arisen, ensuring not only preservation and reproducibility of experimental analyses but enabling reinterpretation studies for the whole HEP community~\cite{LHCReinterpretationForum:2020xtr}. The \rivet tool is the clear choice for (differential) measurements, particularly where detector effects have been unfolded to a fiducial phase-space by the original experiment (and the statistical consequences fully reported, cf. Section~\ref{data-products}). The LHC experiments have established \rivet-based measurement-preservation programs, and similar efforts are gaining traction for heavy-ion experiments, e.g.~STAR and PHENIX programmes based around \rivet's heavy-ion features. Such initiatives need to be built into the data publication and exploitation plans of ongoing and future experiments, to maximise the scientific impact of the analyses.

%Reconstruction-level analyses are 
Analysis preservation at the reconstruction-level is 
more ambiguous, as some form of detector response needs to be encoded, and this will necessarily be less accurate than the original detector simulation+reconstruction code. \rivet incorporates a transfer-function approach for experiment-provided efficiencies and kinematic distortions to be applied to generator-level events but currently contains few reconstruction-level (usually BSM-search) analyses.

%\madanalysis and ColliderBit provide similar systems (and an interface to alternative fast-simulation using \delphes), and have more extensive databases of implemented analyses; \checkmate also uses \delphes, but now focuses more on long-lived particle searches; and experiments can also publish analyses in lightweight, executable form such as ATLAS' semi-public SimpleAnalysis system (where analysis logic is made public, but the detector transfer-functions currently are not).
In parallel, driven by the need of open access to BSM recasting, the theory community has been developing various simulation-based reinterpretation frameworks for reconstruction-level analyses, in particular \checkmate~\cite{Drees:2013wra,Dercks:2016npn}, \madanalysis~\cite{Dumont:2014tja,Conte:2018vmg} and GAMBIT's ColliderBit~\cite{GAMBIT:2017qxg}. They either also use a transfer-function approach, or rely on \delphes~\cite{deFavereau:2013fsa} for the emulation of detector effects; in some cases also generator-level events are used. A detailed overview is given in \cite{LHCReinterpretationForum:2020xtr}.
%
Experiments can also publish analyses in lightweight, executable form such as ATLAS' semi-public \simpleanalysis system %(where analysis logic is made public, but the detector transfer-functions currently are not).
(In particular, the analysis logic but not necessarily reconstruction efficiencies, and more.)
%
%The natural consolidation of lightweight frameworks into orthogonal domains has been welcome, reducing the ambiguity over which public tool is most suitable for an experiment's preservation program. 

As in the cases of full-detail analysis preservation and \rivet-based lightweight measurement preservation, active policy and support efforts will be required in experiments as part of the publication process to ensure the required level of preservation coverage. An example of good practice in this respect is the CMS jets+missing energy analysis \cite{CMS:2021far}, which provided a \madanalysis recast code \cite{Albert:2774586,DVN/IRF7ZL_2021} together with the paper, and in ATLAS and CMS' established integration of \rivet-analysis release into the publication process of suitable data-analyses.

All current tools support standard MC-generator tools such as the HepMC event record. However, essential features such as propagation of systematic uncertainties via weight vectors are not wholly or consistently implemented, and MC generators also have not yet standardized weight-coding conventions. Placing emphasis again on standard interfaces will help speed the convergence of the reconstruction-level BSM-search tools in particular.

% \COMMENT{
% From Raghav in STAR:
% I can comment on the analysis part. We did officially bless and release a few of the analyses along with our Pythia~8 tune paper: \url{https://github.com/star-bnl/star-pythia8-tune}.
% These could in principle be included in the next version of \rivet.
% }

% \TODO{Gambit ColliderBit, CheckMATE, MadAnalysis5\dots more? As archives of BSM-search analysis logic. Detail is usually limited, but precision is not essential. Detector emulation through Delphes or object smearing + efficiencies. CheckMATE emphasis shifted toward LLPs, using experiment-published efficiency maps.}

% \TODO{Mention e.g.~Contur and MA Reint as reinterpretation layers on lightweight preservations. SK: yes for Contur, but in MA5 and Checkmate, this is integrated into the tool, see also text modifications above}

% \TODO{Experience of experiment preservation policy in principle and practice: ATLAS and CMS official programs and semi-enforced linking of \rivet coding to the publication process. Without incentives, extra work is skipped, and implementations from outside the experiment are less valuable. Increasing use in Higgs (preservation and EFT interpretation) and HI.}

% \TODO{Stress need for better meta-data stewardship for analysis codes, to make them searchable and findable (also an issue in \rivet)}

\COMMENT{AB: As the picture has been evolving, we should presumably say something about the public availability and robustness of MVAs, which lie somewhere between ``data products'' and ``analysis logic''. As well as technical challenges in the long-term preservation of methods based on evolving frameworks, the questions of the applicability (and uncertainty propagation) from applying MVAs to smeared or truth-level MC events are more urgent for multivariate techniques than for simple cut-sets. Feeds back to methodology in the MVA training and its reporting and reproducibility.
\hrulex %
SK: yes! There will also be another Snowmass white paper by T.~Plehn et al.\ which is relevant in this context.
\hrulex %
AB: great! So we don't need to go into much detail, but I think lightweight preservation (either particle or reco-level) is the right place to raise this, as it's there that the issues of ML-framework versioning/compatibilities, and validity of the input features are acute, both for external and internal tools. Full-detail preservation can preserves the whole version-specific toolset used for each analysis, so avoids many of the issues. %
}

\subsection{Analysis description languages}

Both full-detail and lightweight preservation frameworks have the significant benefit of being executable, %for reusability. However, neither is necessarily \emph{readable} as clear documentation of the analysis logic. In full-detail preservation, this logic gets encoded in arbitrarily many scripts and codes within the preserved containers and repositories; in lightweight preservation, the analysis code's readability varies greatly between frameworks and individual analysis encodings.

Therefore, the third facet of analysis preservation is to encode an abstract description of the analysis logic in a clear, human-readable, and unambiguous form-- as should be the case in the paper text but rarely given in practice to the level of detail required for full reproduction. Such a format need not compete with other approaches. It could be used internally as a step in the experimental analysis, be used directly in papers or generate paper elements, and as a configuration file or source for translations to code in each lightweight framework.

As with declarative workflow description formats, a recent trend is to decouple analysis logic from the software frameworks used to execute it. Many new-generation analysis frameworks\todo{which?} accumulate all information related to object and event processing in a single location, e.g.~in, a configuration file, at least for part of the processing chain. An approach that takes this one step further is to use domain-specific rules or even languages (DSLs) dedicated to expressing the physics content of HEP analyses. DSLs can either rely on the syntax of an existing computer language (embedded DSL) or have a custom syntax (external DSL) more tailored to the semantics of the HEP-analysis context. Again, if possible, a declarative approach has many benefits due to abstracting practical details into framework implementations. Several DSL approaches are studied, and various working prototypes exist. \TODO{To be continued and refined...}


% % Sezen
% The infrastructures provided by the experiments for preserving analyses constitute a crucial step in analysis preservation. However, an equally important step is to preserve this information in an accessible and easily communicable manner. The thousands of analyses designed by experimental collaborations and phenomenologists have accumulated a tremendous source of physics content. This diverse content can inspire and inform new analysis ideas or train the new generation of particle physicists. Therefore the physics content of analyses should be presented explicitly, in full detail, yet in a sufficiently clear and systematic manner.

\TODO{Incompleteness issue: HEP analyses are not fully domain-specific and often need to execute custom logic in addition to predefined behaviors. To cope, either we extend the DSL to incorporate new known behaviors providing an ``escape'' mechanism to inject general-purpose code (compromising its framework independence) or be combined with pre-and post-processing code to cover gaps in the DSL capabilities. Are there identified routes forward on these issues?}

% self-documenting
% Allow querying analysis information

% ADL/CL
%\cite{Brooijmans:2016vro,Sekmen:2018ehb,Unel:2021edl}
% NAIL, Fast, Jim's codes

\QUESTION{What are public documents that can be cited heavily here from the collaborations?}

\QUESTION{What is the difference between public information and publication of public analysis products (e.g., \hepdata)?
Is this where Sezen's point on communication on the types of analyses comes in?}


% \subsection{Preservation tools}

% \TODO{SK: to my mind, this is taken care of in the previous sections so that section 3.4 can be removed.\qquad AB: agree!}

% \ALERT{The following are just a brain dump of tools. These are going to get removed or just mentioned in a sentence, but are more here for the time being to make sure that we think about what \textbf{concepts or goals} these tools represent.}

% \begin{itemize}
%     \item \recast
%     \item ATLAS \simpleanalysis
%     \item \rivet (Andy can focus here)
%     \item CMS Analysis Preservation Database (currently internal)
%     \item CMS Analysis Description Language (ADL)
%     \item \ldots
% \end{itemize}

% \QUESTION{At the moment, is there any document that links all of these together? If not, can we link them very concisely here?}
% \TODO{RiF status \& recommendations report? SK: no, not really: the RiF report focuses on data products needed for reproducibility and public recasting tools, not [collaboration-internal] analysis preservation per se.}

\TODO{Need to rework recommendations as discussed: a first point on the crucial role of common standards and interoperability to reduce fragmentation and duplication of effort; followed by specific statements on concrete actions needed in full-detail and lightweight preservation, and for completion/uptake of ADLs. Points 3.3 and 3.4 are currently too long and should be merged; all points should be less vague.}

\noindent
\textbf{Recommendations:}
\begin{description}
   \item[3.1:] Use software version control and archiving systems for long-term preservation beyond the experiment lifetime.
   During the lifetime of the experiments, this is the responsibility of the experimental collaborations and, beyond that, one of the host laboratories.
   \item[3.2:] Encourage the support and use of common and portable analysis tools, making analyses more reproducible.
    % 
    % Kyle: It seems there are two ideas here. One is to simplify / approximate analysis done in current approaches. Other is to develop analysis tools (e.g., declarative specifications) that are easier to use, more portable, and reproducible. 
   \item[3.3:] Ensure the preservation of analysis workflows that capture the complete computational processing chain available to the experiments whenever possible.
   \textbf{Recommendation summary from Sezen - can merge parts into 3.2:} Explore ways to filter the physics content of analyses and present them explicitly in full detail using a clear, concise, and systematic description for preservation. The description should be self-documenting and easily recognizable and understandable by particle physicists from different domains and generations.  
   \item[3.4:] Describe physics analyses in a declarative, executable, and translatable form so that they are reproducible independent of the experiment software.
   % Sezen: Make sure the analyses can be communicated as clearly as possible.
   % Sabine: This also improves knowledge transfer.
   % Andy: I think this part needs to be more explicit about analysis logic in a operational form that complements the full-detail preservation of Item~3.2, i.e.~in, a form that can either be run directly on MC events or can be translated into a runnable form (cf.~ADL).
   % Matthew: How to describe things like ADL (declarative) and \rivet (a specific implementation) with common words (not saying these things by name)?
\end{description}


%-------------------------------------------------------------
\section{Preservation of data products}
\label{data-products}
%-------------------------------------------------------------

\noindent\textbf{Writers:} Lukas, Sabine, Giordon, Andy Buckley, Graeme Watt \\

There is widespread consensus in the community that experiments should systematically provide all relevant derived data or \emph{data products} in an open-access numerical form for future reuse~\cite{LHCReinterpretationForum:2020xtr}. 
Such data products are also called publication-related or ``Level 1'' data in the 
\href{https://opendata.cern.ch/docs/about}{DPHEP categories}. 
They include observed and expected event counts  (including error sources), efficiency functions,  bin-to-bin correlations, profile likelihoods, full statistical models, signal efficiencies, simplified model results, and much more, 
as discussed extensively in Refs.~\cite{LHCReinterpretationForum:2020xtr,Cranmer:2021urp}.
%\emph{What} should be provided to enable a pertinent re-use is discussed extensively in Refs.~\cite{LHCReinterpretationForum:2020xtr,Cranmer:2021urp}, so we do not go into more details here. 

%Moreover,  \hepdata has long been established as the platform of choice for this purpose by the large collaborations.
% G. Watt: added the paragraph below on HEPData scope, infrastructure, and limitations, hastily written. Happy to provide more input if requested.
With regards to publication infrastructure, 
\hepdata~\cite{hepdata} is the primary open-access repository for %publication-related (``Level 1'')
data products from particle physics experiments, with a long history going back to the 1970s.
% The \hepdata project underwent a complete transformation in 2017 to a new platform (\url{hepdata.net}) hosted on CERN computing infrastructure~\cite{hepdata}.
% Another transition was made in 2020 to deploy the web application via a Docker image on a Kubernetes cluster shared with the INSPIRE-HEP project.
Funding is provided by the UK Science and Technology Facilities Council (STFC) to Durham University (UK) for staff to maintain the operation of the \href{https://www.hepdata.net/}{hepdata.net} site, provide user support, and develop the open source software (available on the \href{https://github.com/HEPData}{\hepdata GitHub organisation}) underlying the web application.

In the past, \hepdata staff at Durham University handled data preparation in a standard format and uploaded it to the repository.
However, now these tasks are delegated to the experimental collaborations.
Data submitted to \hepdata (as YAML) is primarily in a tabular form that can be interactively plotted in the web application and automatically converted to other standard formats (i.e. CSV, JSON, ROOT, YODA).
The interactive nature of \hepdata means that data tables must be kept sufficiently small ($\sim$MB or less) that they can render in a web browser.
In practice, tables with more than $\sim 10,000$ rows (for example, a covariance matrix for a measurement with $\sim 100$ bins) cannot efficiently render in a web browser.
However, moderately large tables or non-tabular data files can be attached to a \hepdata record as additional resources (in any format).
The original files are downloadable, but the interactive nature is lost.
\hepdata imposes an overall size limit of 50 MB on the uploaded archive file to avoid problems caused by the attempted upload. 
%
%Publication-related (``Level 1'') high-energy physics data that is 
Data products that are not suitable for \hepdata, due to either being too large or predominantly in a non-tabular format, might be submitted to another data repository like \href{https://zenodo.org/}{Zenodo}.
Zenodo currently plugs a gap to host data (and software) that does not fit into other repositories.%
\footnote{This concerns also model files, Monte Carlo simulation, and all kinds of data products from phenomenological studies.}
A new HEP-specific instance of Zenodo could perhaps better serve the particle physics community in the future.


Although many experimental collaborations\footnote{The LHC ATLAS, CMS, ALICE, and LHCb experiments, and increasingly also in the heavy-ion community e.g.~STAR, PHENIX, and NA61/SHINE.} see the provision of their results on \hepdata together with the published paper as standard procedure, in practice the coverage remains incomplete.
\TODO{Here elevate the footnote to emphasize the cultural value to the HEP community.
LHC community has had a large win here.
Heavy Ion community is starting to have a culture shift here, and this should be encouraged and supported and funded as needed.}

Often, instead of being provided on \hepdata, digitized results/plots are available only on collaboration web pages, without appropriate documentation, versioning, or other data stewardship.
Sometimes, the linked ROOT files are wrong (not corresponding to the associated plot) or otherwise corrupted.
Moreover, the digital material is missing altogether.
Experience shows that missing or wrong resources can rarely be retrieved or corrected, as often the analysis team has disbanded with analyzers leaving the field, or the relevant files become lost. 

As with analysis preservation, current heavy-ion experiments have less comprehensively established procedures for data-product preservation in \hepdata than their LHC counterparts, but such procedures are now integrated into the publication processes for STAR and PHENIX, and decoupled from publication in sPHENIX. Coverage and process for BRAHMS and Phobos is less developed. STAR and PHENIX have both also instituted programmes of transcription of previously published data to \hepdata, including as a form of experimental shift in 2020-22. Ensuring systematic and sufficiently detailed preservation of analysis products from all experiments in a central subject database such as \hepdata is a central component of maximising scientific impact and data re-use, and should be designed into new experiment investment and deliverables from the start.\todo{As with all the preservation modes described in this doc, but doesn't hurt to say it again\dots or maybe just in the recs.}

\TODO{Need to revise this into other sections so that it is coherent.}
Part of the problem is missing time and community recognition to providing material on \hepdata.
The \href{https://indico.cern.ch/category/14155/}{RAMP} (``Reinterpretation: Auxiliary Material Presentation'') seminar series aims at providing more visibility and recognition for such efforts,  but more is needed. 

Regarding \hepdata itself, material beyond digitized plots (like MC run cards, input files for benchmark points, or, most importantly, statistical models) are ``additional resources'', often lumped together in compressed archives without any standard structure.
The types of data products being preserved has become much richer and diverse than flat tables.
To be able to provide the necessary infrastructure for all of these data products will require additional funding.
There is room, and a clear need, for ``FAIR''-ification of these precious data products. 

\noindent
\textbf{Recommendations:}
\begin{description}
  \item[4.1:] Make the provisioning of all data products associated with an experimental analysis a mandatory step for publication.
  This will require establishing appropriate person power, time, and community recognition for that effect.
  %\ALERT{Kyle: We might want to have first a statement about capture and preservation even if it is not made public and then follow up with recommending it to be public.}
   \item[4.2:] Assure appropriate resources and funding for further development of the cyberinfrastructure, such as \hepdata and other repositories like Zenodo, to preserve the data products and metadata, and extend the current data structure to include more rich data products and information beyond paper plots and flat tables, e.g., statistical models, in an individually searchable and citeable form.
   \ALERT{Kyle: it would be nice to incorporate INSPIRE, CAP, PDG, and other cyberinfrastructure components.
   \hrulex
   Matthew: @Kyle we might need you to comment a bit more on how you see things like INSPIRE and CAP fit in here, beyond just that they need more funding.
   If so, maybe we should work a funding story for them into another section?}
   % Maybe later on in revisions, we can make these a few sentences to make the FAIR components of them more transparent.
\end{description}

% \NB{Lukas: Focusing on the funding agent view, it would be good to focus on the types of data being preserved has become \emph{much} richer --- we have moved far beyond flat tables.
% Need funding and infrastructures to realize its use and make it interactive.}

\TODO{Reword recommendations language-wise, to make them read nicely.}


\hrule 


% \COMMENT{Christine Nattrass comments:  
% RHIC:  ALICE, STAR and PHENIX now require \hepdata with publication now. I proposed it for sPHENIX; it was decided that the mechanism for how the data points were distributed would be a procedure, not in the publication policy, but as a practical matter, I think \hepdata will be adopted by sPHENIX. Phobos and BRAHMS do not officially have a procedure to get data up. BRAHMS has some effort which is less formal. Phobos only has two papers up on \hepdata. (Note Phobos is not an acronym, hence not being all caps.)  That is, overall, the heavy ion community has adopted \hepdata - but we still have issues with previously published papers.

% For previously published papers:  STAR made formatting data from old papers for \hepdata an alternative to shifts during the pandemic.

% In PHENIX, I just put in a proposal to hire undergrads to do this. Here's some text out of that proposal, with light editing for coherence:

% \hrulex
% \hepdata only became obligatory for STAR in 2019 and PHENIX in 2020, after a proposal by Nattrass to the PHENIX Executive Council. Around this time, both experiments' web sites suffered prolonged outages due to cybersecurity incidents, making uploading data to \hepdata a much higher priority. In early 2020, it was determined that the old PHENIX web site hosting data would no longer be externally available.  \hepdata is now the \textit{only} means of disseminating PHENIX data. Nattrass helped develop a procedure for submission of PHENIX data to \hepdata, is helping oversee the submission of PHENIX data to \hepdata (including previously published data), and is assisting primary authors with formatting data. Data from previously published papers, however, are still generally unavailable outside of the collaboration.

% As of January~12, 2022, there are 224 published PHENIX papers and 55 PHENIX papers (25\%) with data available on \hepdata. The majority of papers without data available on \hepdata are earlier papers, including many seminal results in the field. The previously-public PHENIX web page where the data were is still available to PHENIX collaborators internally, but there was no enforced structure. The varied nature of the papers, as well as some issues with the clarity or completeness of  these data, means that there is no trivial, automated way to convert the data to the format required for submission to \hepdata. Furthermore, the preparation of the YAML files for \hepdata requires some knowledge of high energy physics. This task is currently being done voluntarily by PHENIX collaborators, who must learn how to format data for \hepdata, and parasitically by undergraduates working on \rivet. It is therefore very slow.

% Nattrass has supervised several undergraduates as they implement this process, including several with limited programming experience. We estimate that it takes between 2--20 hours for an experienced worker to format data for \hepdata, depending on the complexity of the paper. The simplest papers may only have one or two data points, while at least one paper formatted by our group had over 2000 data tables with about 20 data points each. A beginning undergraduate would need to learn some basics of the field, and could use the materials developed for the CURE. We estimate this would take approximately 45 hours, commensurate with the time it takes in the CURE, and that we could complete approximately 100 papers by hiring two undergraduates part time during the semester and full time in the summer. This task is scalable, so if less funding is available, it would still be possible to format some PHENIX data for upload to \hepdata.
% \hrulex

% Successfully uploading data to \hepdata also requires extensive communication with and contributions from current and former PHENIX collaborators, both to resolve questions about the data and to vet the final product before it is made public. Most people are incredibly enthusiastic to have someone else format data from a measurement they worked on, and the collaboration membership and leadership strongly support this effort. We have not had problems getting assistance in the past and do not anticipate problems in the future.
% }


% -------------------------------------------

%-------------------------------------------------------------
\section{Recasting and reinterpretation}
\label{reinterpret}
%-------------------------------------------------------------

\noindent\textbf{Writers:} Sabine, Matthew, Andre, Clemens\\

The physics impact of an experimental analysis can be increased well beyond its original purpose through reinterpration of its results within new physics scenarios.
For this to be possible, the preservation of data products and analyses (usually in a lightweight format) are essential, as discussed in Secs.~\ref{analysis-preservation} and \ref{data-products}.
In addition, reinterpretation of several analyses within a given BSM scenario is relevant for displaying the complementarity of distinct searches as well as identifying possible gaps in coverage.
Such gaps can then be used as motivation for designing new experimental searches.


Reinterpretations can be based on analysis preservation published by the experimental collaborations (e.g. ATLAS' SimpleAnalysis and CMS aproved MadAnalysis5 code) or on analysis recasting developed outside the collaborations, see Sec.~\ref{lightweight}. Below we briefly review these two possibilities and highlight some of the current issues faced by each approach.
We also point out some of the steps which could be taken to improve the current status.

%\NB{Can lean heavily on the reinterpretation report here and cite that}

%\subsection{Principles}

\subsection{Collaboration-internal recasting}

\TODO{Matthew and Clemens will revise an experimental viewpoint from ATLAS and CMS}

\NB{This section can probably get covered in other sections like \S 3.2, so maybe try to get this information covered first elsewhere.}

\subsubsection{CMS-internal recasting}

Within CMS, reinterpretation or recasting is largely performed within ongoing analyses or via the statistical combination of completed analyses. While the analysis code is not systematically archived, the data products for signal extraction are usually preserved so that uncertainties across different analyses can be treated consistently in a statistical combination. Furthermore, for some analyses, see e.g.\ Refs.~\cite{CMS:2017may,CMS:2018tuo}, cut efficiency tables are provided for further reinterpretation. For several other analyses, simplified likelihoods are also published~\cite{CMS:2017nxf,CMS:2017zts,CMS:2018ffd,CMS:2018eqb}.

\begin{itemize}
    \item \href{https://github.com/recast-hep/recast-atlas}{recast-atlas}
        \begin{itemize}
            \item \NB{Currently have \texttt{recast-atlas} under collaboration-internal instead of public, as even though the code is open source you still need EOS access at CERN. So it is more ``CERN public'' than ``true public''.}
        \end{itemize}
    \item \ldots
\end{itemize}

Mention difficulties currently met when attempting global reinterpretations (e.g., pMSSM efforts, EFT fits) to make recommendations on improvements.

\COMMENT{SK: mention some actual reinterpretations done inside the collaborations in order to illustrate use and benefits?}

%\subsection{Public recasting tools}
\subsection{Recasting outside the experimental collaborations}


% Collaborations can increase a given analysis's impact by publicly making software frameworks available for reinterpreting the analysis results within distinct new physics scenarios. 

% \ALERT{Kyle: Suggest we have some way of distinguishing between the ``official" recasting tools that the experiments would endorse and ``unofficial'' recasting tools that are useful but not necessarily endorsed by experiments. I would not use the word "public" to make that distinction because the RECAST API could be made public even if the underlying processing of the data is done internally inside the experiments. }
% \COMMENT{SK: we can make the distinction between collaboration-internal or collaboration-owned and collaboration-independent efforts (?)}

Currently, several software tools (CheckMATE, MadAnalysis5, Rivet/Contur, SModelS, and others) developed outside the collaborations are publicly available for the task of recasting and reinterpretation. These tools typically supply a database of implemented analyses, but also allow the user to implement new ones.
In addition they provide the tools for making statistical statements about the results, either by implementing the statistical models supplied by the collaboration (if available) or by taking some simplifying assumptions.
These tools rely on distinct reinterpretation approaches, as discussed in Section~III of \cite{LHCReinterpretationForum:2020xtr}.
Maintaining these tools and implementing new analyses requires considerable person power and funding.

The public tools provide distinct analyses coverage and sometimes different implementations of the same analysis.
Due to the proliferation of analyses and tools, it is often the case that reinterpretation efforts do not make the most of the available framework due to the lack of interoperability between the tools.
Within this context, it would be desirable to coordinate the recasting effort to maximize its impact and avoid redundant work. 
As a final goal, a unified format for analysis implementation, which could be used interchangeably by the different tools, would significantly improve the reinterpretation potential of the phenomenology community.
Even though this proposal has been discussed in the past (refs?), not enough progress has been made.
A few steps could be taken by the community with immediate benefits and which could help achieving this final goal.
In particular, a collaborative effort among the tool developers could maintain a central location for bookkeeping the list of implemented analysis in the specific tools and the corresponding validation material.
An illustration of such an effort can be found within the 
long-lived particle recasting community, which created a centralized location for analysis preservation and validation material in the format of a GitHub repository\cite{llpRepo}.
%This repository has been shown to be useful for the community and helped to include the recasting of LLP searches in public tools.
In addition, whenever possible, it would be helpful to adopt a few validation guidelines to allow for a proper estimate of the recasting uncertainties introduced by each analysis implementation.
As a second step, adopting basic standards for the input and output formats would also help the user to efficiently use distinct tools.
\COMMENT{AL: What else? For me, it seems that achieving a common analysis implementation code would require some big steps, such as rewriting all the existing tools, writing parsers, keeping the backwards compatibility, etc. I couldn't come up with small steps to be taken here.}

Another vital aspect of reinterpretation is the statistical treatment of the results. In recent years, the amount of information provided by experimental collaborations has increased significantly, allowing for a more robust statistical interpretation in phenomenological studies. However, to take full advantage of these new developments, it would be desirable to coordinate and unify the statistical output format and treatment within the specific recasting tools to have a common ground for comparison. Another critical aspect to be taken into account is the possibility for a global analysis of the results, which could, in principle, combine results from different recasting tools. The GAMBIT Collaboration has done this~\cite{Kvellestad:2019vxm} and seen more recently by the ``protomodelling'' project in \cite{Waltenberger:2020ygp}.
These studies would also benefit from a unified statistical treatment in the recasting tools. 
To help in this direction the statistical analysis should be factorized as much as possible within each tool, allowing for interoperability. In addition,  standards and guidelines could be established for presenting the results and providing the required output for the statistical interpretation.
\COMMENT{AK: The above paragraph maybe gives the impression that \cite{Kvellestad:2019vxm} and \cite{Waltenberger:2020ygp} are examples of studies that combine results from different recasting tools, which I don't think was the intended meaning? Suggested edit: \textit{Another critical aspect to be taken into account is the possibility for a global analysis of the results. The GAMBIT Collaboration has done this~\cite{Kvellestad:2019vxm} and seen more recently by the ``protomodelling'' project in~\cite{Waltenberger:2020ygp}. Such global analyses could, in principle, combine results from different recasting tools.} Also, perhaps you meant to cite the GAMBIT electroweakino study~\cite{GAMBIT:2018gjo}, not the review paper~\cite{Kvellestad:2019vxm}?}

A further motivation for facilitating collider reinterpretations in global analyses is that it enables large-scale and adaptive exploration of the complementarities between collider results and other experimental results. Such global fits have the potential of both uncovering gaps in the experimental coverage, and identify which uncovered BSM scenarios are most plausible in light of other experiments, and thus constitute well-motivated targets for future analyses/experiments. Realising this potential, however, depends critically on computationally efficient approaches to analysis reinterpretation, since a global fit faces the additional computational cost of recasting all relevant non-collider results and explore a typically many-dimensional theory parameter space. As such, a focus on code efficiency, stability and parallelisability in reinterpretation tools, and on development of fast approximations for expensive computations (e.g.~computation of higher-order cross-sections or marginalisation of many-parameter statistical models), is important to enable proper utilisation of experimental results.

%One possible solution for establishing a more robust and consistent statistical interpretation would be to share between the recasting tools a single software package for the statistical calculations required since these are common for the typical reinterpretation studies.
%Some tools, such as SModelS and MadAnalysis5, already share code for a few steps of their statistical analysis, but it is not a centralized effort. An example of such a multi-purpose statistical package is pyhf~\cite{pyhf}, which is publicly available and open source. With some coordination and further development, pyhf or a similar package could be used and maintained by the developers' reinterpretation tools to have a common statistical interpretation of the results produced by each tool.


When it comes to reinterpretation of analyses results, the effort is often non-trivial and demands significant computing time. Therefore it is important to ensure reproducibility and  preservation of the results obtained.
The same platforms used for data product preservation (see Sec.~\ref{data-products}) can also be used by the theory community to preserve the reinterpretation results and data.
In particular, \href{https://zenodo.org/}{Zenodo} has already been used by a few groups to publish auxiliary material from phenomenological studies.
It is also important that the tools used for obtaining the results are properly documented in order to allow for future reproducibility.
This policy should be largely encouraged within the theory community.


% (some lines on motivation and use)
%
% Different reinterpretation methods and public software frameworks that enable them are discussed in detail in section~III of \cite{LHCReinterpretationForum:2020xtr}. We do not repeat this here. However, one important aspect to highlight is the aim of global analyses, which is putting all the available experimental information together. Such global approaches are attempted by, e.g., the GAMBIT collaboration and the ``protomodelling'' project in \cite{Waltenberger:2020ygp}.
% One serious shortcoming is that the set of analyses that can be recast in the different frameworks (CheckMATE, MadAnalysis5, Rivet/Contur, SModelS, and other tools) is still very patchy. It depends a lot on the information available from the experimental collaborations and the person power available in the different teams developing these public frameworks. Often (except for \rivet), these are underfunded theory collaborations. Nevertheless, without Ph.D. students and postdocs(!), such efforts cannot be successful.
%
% Efforts to make recast codes framework-independent / interchangeable between frameworks have been started with the idea of an accord for a  declarative analysis description~\cite{Brooijmans:2016vro} but need much further development (see also Section~3). In particular, ADL parsers for the most common public frameworks (including automatized validation) would greatly benefit but are still missing.
%
%Other points to elaborate:
%\begin{itemize}
%    \item differences between searches and measurements and aspects to be considered when combining them
%    \item machine-learning (MVA recasting still in its infancy)
%    \item efforts regarding EFT interpretations
%    \item preservation of code and data products from theory studies
%\end{itemize}
%
%(finish with recommendations)

\noindent
\textbf{Recommendations:}
    \COMMENT{AL: Maybe the comments about analysis design could fit better when discussing "lightweight" analysis preservation?}
\begin{description}
\item[5.1:] \TODO{Make the point here that designing an analysis with interpretability and reuse in mind requires physics insight early on in the process?}
   %, e.g., things to think about early on so that the signal regions can be orthogonal and easier to combine final states.
   \item[5.2:] Improve the coordination among the different recasting tools with the goals of achieving a centralized framework for recasting analyses and unifying the statistical treatment of the reinterpretation results.
   %interpretations through the setup of a centralized location for documenting the analyses implementations and the unification of the statistical framework used by the tools.
   \item[5.3:] Encourage FAIR-ification of data products from (theory) reinterpretation studies outside the experimental collaborations. %similar to the provision of exp results on HEPData
   % Make clear in the text that this would be archived like Zenodo.
\end{description}
\TODO{Consideration of reuse and reinterpretation during analysis design. }
%-------------------------------------------------------------
\section{Conclusions}
%-------------------------------------------------------------

%-------------------------------------------------------------
\section*{Acknowledgments}
%-------------------------------------------------------------
\TODO{all authors: add your funding}

Sabine Kraml acknowledges support by the IN2P3 master project ``Th\'eorie -- BSMGA'' and the joint ANR-FWF project PRCI SLDNP grant no.~ANR-21-CE31-0023.
Andre Lessa is supported by Sao Paulo Research Foundation (FAPESP) grants no.~2018/25225-9 and 2021/01089-1.



\def\thefootnote{\fnsymbol{footnote}}
\setcounter{footnote}{0}

% Bibliography

\bibliographystyle{JHEP}
\bibliography{bib/preservation,bib/reinterpretation}

\end{document}